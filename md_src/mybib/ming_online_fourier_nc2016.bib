@article{LIN201626,
title = {Online kernel learning with nearly constant support vectors},
journal = {Neurocomputing},
volume = {179},
pages = {26-36},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215014587},
author = {Ming Lin and Lijun Zhang and Rong Jin and Shifeng Weng and Changshui Zhang},
keywords = {Online learning, Kernel machine, Nyström, Sample complexity},
abstract = {Nyström method has been widely used to improve the computational efficiency of batch kernel learning. The key idea of Nyström method is to randomly sample M support vectors from the collection of T training instances, and learn a kernel classifier in the space spanned by the randomly sampled support vectors. In this work, we studied online regularized kernel learning using the Nyström method, with a focus on the sample complexity, i.e. the number of randomly sampled support vectors that are needed to yield the optimal convergence rate O(1/T), where T is the number of training instances received in online learning. We show that, when the loss function is smooth and strongly convex, only O(log2T) randomly sampled support vectors are needed to guarantee an O(logT/T) convergence rate, which is almost optimal except for the logT factor. We further validate our theory by an extensive empirical study.}
}